# -*- coding: utf-8 -*-
"""LinearRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QA-tHm5WHExL2_MckIvJDNRofPISAppx

### , What does R-squared represent in a regression model:

R-squared represents the **proportion of variance in the dependent variable explained by the independent variables**. It indicates how well the regression model fits the data, ranging from 0 to 1.

---

### \$, What are the assumptions of linear regression:

1. **Linearity:** Relationship between predictors and outcome is linear.
2. **Independence:** Observations are independent.
3. **Homoscedasticity:** Constant variance of residuals/errors.
4. **Normality:** Residuals are normally distributed.
5. **No multicollinearity:** Predictors are not highly correlated.

---

### 2, What is the difference between R-squared and Adjusted R-squared:

* **R-squared** measures the proportion of variance explained but **increases with more predictors regardless of relevance**.
* **Adjusted R-squared** adjusts for the number of predictors and only increases if the new predictor improves the model.

---

### E, Why do we use Mean Squared Error (MSE):

MSE measures the **average squared difference between predicted and actual values**, penalizing larger errors more heavily. It is used to evaluate the accuracy of regression models.

---

###, What does an Adjusted R-squared value of 0.85 indicate:

It indicates that **85% of the variance in the dependent variable is explained by the model**, adjusted for the number of predictors.

---

### , How do we check for normality of residuals in linear regression:

* Using **Q-Q plots**
* Conducting statistical tests like the **Shapiro-Wilk test** or **Kolmogorov-Smirnov test**
* Visualizing residual histograms

---

### , What is multicollinearity, and how does it impact regression:

Multicollinearity occurs when independent variables are **highly correlated**, causing unstable coefficient estimates and difficulty in interpreting individual predictors.

---

### , What is Mean Absolute Error (MAE):

MAE is the **average of absolute differences between predicted and actual values**, giving an easily interpretable average error magnitude.

---

### , What are the benefits of using an ML pipeline:

* Automates and streamlines workflow
* Ensures consistent preprocessing
* Facilitates reproducibility and easier deployment
* Simplifies hyperparameter tuning and cross-validation

---

### GC, Why is RMSE considered more interpretable than MSE:

RMSE is the **square root of MSE**, so it is in the same units as the target variable, making it easier to understand the size of errors.

---

### GG, What is pickling in Python, and how is it useful in ML:

Pickling is the process of **serializing and saving Python objects (like models)** to a file so they can be **loaded later without retraining**.

---

### G\$, What does a high R-squared value mean:

It means the model **explains a large proportion of variance** in the dependent variable, indicating a good fit.

---

### G2, What happens if linear regression assumptions are violated:

* Model estimates become biased or inefficient
* Predictions and inference may be unreliable
* Standard errors and hypothesis tests become invalid

---

### GE, How can we address multicollinearity in regression:

* Remove or combine correlated features
* Use dimensionality reduction (e.g., PCA)
* Apply regularization techniques like Ridge or Lasso regression

---

### , How can feature selection improve model performance in regression analysis:

* Reduces overfitting
* Simplifies the model
* Improves interpretability
* Enhances generalization on unseen data

---

### , How is Adjusted R-squared calculated:

$$
\text{Adjusted } R^2 = 1 - \left( \frac{(1 - R^2)(n - 1)}{n - p - 1} \right)
$$

where *n* is sample size and *p* is number of predictors.

---

### , Why is MSE sensitive to outliers:

Because errors are **squared**, large errors (outliers) have a disproportionate effect on the MSE, increasing its value significantly.

---

### , What is the role of homoscedasticity:

Homoscedasticity means the **variance of residuals is constant** across all levels of the independent variables, which is crucial for valid standard errors and hypothesis tests in regression.

### What is Root Mean Squared Error (RMSE):

RMSE is the **square root of the average of the squared differences between predicted and actual values**. It measures the standard deviation of prediction errors and gives an idea of how far predictions deviate from actual outcomes in the same units as the target variable.

---

### \$C, Why is pickling considered risky:

Pickling is risky because it **can execute arbitrary code during unpickling**, making it vulnerable to security exploits if the pickle file comes from an untrusted or malicious source. Also, pickled objects may not be compatible across different Python versions or environments.

---

### \$G, What alternatives exist to pickling for saving ML models:

* **Joblib:** Optimized for serializing large numpy arrays, safer and faster than pickle in some cases.
* **ONNX (Open Neural Network Exchange):** Standard format for ML model interoperability across frameworks.
* **TensorFlow SavedModel / PyTorch's TorchScript:** Framework-specific model serialization.
* **JSON or YAML:** For saving model parameters or configurations in a human-readable format.

---

### , What is heteroscedasticity, and why is it a problem:

Heteroscedasticity occurs when the **variance of residuals is not constant across all levels of independent variables**. It violates the linear regression assumption of homoscedasticity, leading to **inefficient estimates and invalid standard errors**, which affects hypothesis testing and confidence intervals.

---

### , How can interaction terms enhance a regression model's predictive power?

Interaction terms allow the model to **capture the combined effect of two or more variables on the dependent variable**, beyond their individual effects. This can improve the model's ability to represent complex relationships and increase prediction accuracy.

### 1. Visualize the distribution of errors (residuals) for a multiple linear regression model using Seaborn's "diamonds" dataset.

```python
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load diamonds dataset
diamonds = sns.load_dataset('diamonds')

# Use numeric features only for simplicity, drop NA if any
diamonds = diamonds.dropna()
X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]
y = diamonds['price']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train linear regression
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Calculate residuals
residuals = y_test - y_pred

# Plot residuals distribution
sns.histplot(residuals, kde=True)
plt.title('Distribution of Residuals')
plt.xlabel('Residuals')
plt.show()
```

---

### 2. Calculate and print MSE, MAE, and RMSE for a linear regression model.

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Assuming y_test and y_pred from previous example

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
```

---

### 3. Check linear regression assumptions with plots (linearity, homoscedasticity, multicollinearity)

```python
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Linearity: scatter plot of predicted vs actual
plt.scatter(y_pred, y_test)
plt.xlabel('Predicted values')
plt.ylabel('Actual values')
plt.title('Predicted vs Actual values (Linearity check)')
plt.show()

# Residual plot for homoscedasticity
residuals = y_test - y_pred
plt.scatter(y_pred, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Residuals vs Predicted values (Homoscedasticity check)')
plt.show()

# Correlation matrix for multicollinearity
corr_matrix = X.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix (Multicollinearity check)')
plt.show()
```

---

### 4. Create a machine learning pipeline with feature scaling and evaluate different regression models

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import cross_val_score

models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.1)
}

for name, model in models.items():
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', model)
    ])
    scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=5)
    rmse_scores = np.sqrt(-scores)
    print(f"{name} RMSE: {rmse_scores.mean():.2f} (+/- {rmse_scores.std():.2f})")
```

---

### 5. Simple Linear Regression model: print coefficients, intercept, R-squared

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Using 'carat' to predict 'price'
X_simple = diamonds[['carat']]
y_simple = diamonds['price']

X_train, X_test, y_train, y_test = train_test_split(X_simple, y_simple, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

print(f"Coefficient (m): {model.coef_[0]:.4f}")
print(f"Intercept (c): {model.intercept_:.2f}")
print(f"R-squared score: {model.score(X_test, y_test):.4f}")
```

---

### 6. Analyze relationship between total bill and tip in 'tips' dataset and visualize simple linear regression

```python
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import numpy as np

tips = sns.load_dataset('tips')
X = tips[['total_bill']]
y = tips['tip']

model = LinearRegression()
model.fit(X, y)

# Predictions for line
x_range = np.linspace(X.min(), X.max(), 100).reshape(-1,1)
y_pred = model.predict(x_range)

plt.scatter(X, y, label='Data Points')
plt.plot(x_range, y_pred, color='red', label='Regression Line')
plt.xlabel('Total Bill')
plt.ylabel('Tip')
plt.title('Total Bill vs Tip Regression')
plt.legend()
plt.show()
```

---

### 7. Fit linear regression on synthetic data, predict new values, plot data and regression line

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Synthetic data
X = np.random.rand(100, 1) * 10  # feature
y = 2.5 * X + np.random.randn(100, 1) * 2  # target with noise

model = LinearRegression()
model.fit(X, y)

# Predict
X_new = np.array([[0], [10]])
y_new = model.predict(X_new)

plt.scatter(X, y, label='Data Points')
plt.plot(X_new, y_new, color='red', label='Regression Line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression on Synthetic Data')
plt.legend()
plt.show()
```

---

### 8. Pickle a trained linear regression model and save it

```python
import pickle

# Using the model from previous example
filename = 'linear_regression_model.pkl'
with open(filename, 'wb') as file:
    pickle.dump(model, file)

print(f"Model saved as {filename}")
```

---

### 9. Fit polynomial regression (degree 2) and plot regression curve

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Synthetic data
X = np.sort(np.random.rand(100, 1) * 10, axis=0)
y = 0.5 * X**2 + 2 * X + 1 + np.random.randn(100, 1) * 5

# Transform features
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Fit model
model = LinearRegression()
model.fit(X_poly, y)

# Predict
X_fit = np.linspace(0, 10, 100).reshape(100, 1)
X_fit_poly = poly.transform(X_fit)
y_fit = model.predict(X_fit_poly)

plt.scatter(X, y, label='Data Points')
plt.plot(X_fit, y_fit, color='red', label='Polynomial Regression Curve')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Polynomial Regression (Degree 2)')
plt.legend()
plt.show()
```

### 4. Use VIF to check multicollinearity

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Assume X is your DataFrame of features
vif_data = pd.DataFrame()
vif_data['feature'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
print(vif_data)
```

---

### 15. Synthetic data for degree 4 polynomial, fit, and plot

```python
X = np.sort(np.random.rand(100, 1) * 10, axis=0)
y = 1.5 * X**4 - 10 * X**3 + 5 * X**2 + 3 * X + 7 + np.random.randn(100, 1) * 100

poly = PolynomialFeatures(degree=4)
X_poly = poly.fit_transform(X)

model = LinearRegression().fit(X_poly, y)

X_fit = np.linspace(0, 10, 100).reshape(-1,1)
X_fit_poly = poly.transform(X_fit)
y_fit = model.predict(X_fit_poly)

plt.scatter(X, y)
plt.plot(X_fit, y_fit, color='red')
plt.show()
```

---

### 16. ML pipeline with standardization + multiple linear regression + print R²

```python
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', LinearRegression())
])
pipeline.fit(X_train, y_train)
print("R-squared:", pipeline.score(X_test, y_test))
```

---

### 17. Polynomial regression (degree 3) on synthetic data and plot

```python
X = np.sort(np.random.rand(100, 1) * 10, axis=0)
y = 2 * X**3 - 3 * X**2 + 5 * X + np.random.randn(100,1)*10

poly = PolynomialFeatures(degree=3)
X_poly = poly.fit_transform(X)

model = LinearRegression().fit(X_poly, y)

X_fit = np.linspace(0,10,100).reshape(-1,1)
X_fit_poly = poly.transform(X_fit)
y_fit = model.predict(X_fit_poly)

plt.scatter(X, y)
plt.plot(X_fit, y_fit, color='red')
plt.show()
```

---

### 18. Multiple linear regression on synthetic data with 5 features, print R² and coefficients

```python
X = np.random.rand(100, 5)
y = X @ np.array([1.5, -2, 3, 0, 4]) + np.random.randn(100)

model = LinearRegression().fit(X, y)
print("R-squared:", model.score(X, y))
print("Coefficients:", model.coef_)
```

---

### 19. Generate synthetic data for linear regression, fit model, plot points and regression line

```python
X = np.random.rand(100,1) * 10
y = 3 * X + 5 + np.random.randn(100,1) * 3

model = LinearRegression().fit(X, y)
plt.scatter(X, y)
plt.plot(X, model.predict(X), color='red')
plt.show()
```

---

### 20. Synthetic dataset with 3 features, multiple linear regression, print R² and coefficients

```python
X = np.random.rand(100,3)
y = X @ np.array([2, -1, 3]) + np.random.randn(100)

model = LinearRegression().fit(X, y)
print("R-squared:", model.score(X, y))
print("Coefficients:", model.coef_)
```

---

### 21. Serialize and deserialize ML model using joblib

```python
from joblib import dump, load

dump(model, 'model.joblib')
loaded_model = load('model.joblib')
print(loaded_model.predict(X[:5]))
```

---

### 22. Linear regression with categorical features using one-hot encoding ('tips' dataset)

```python
tips = sns.load_dataset('tips')
X = pd.get_dummies(tips[['sex', 'smoker', 'day', 'time']], drop_first=True)
y = tips['total_bill']

model = LinearRegression().fit(X, y)
print("Coefficients:", model.coef_)
```

---

### 23. Compare Ridge vs Linear Regression, print coefficients and R²

```python
X = np.random.rand(100, 3)
y = X @ np.array([1.2, -3, 2]) + np.random.randn(100)

lr = LinearRegression().fit(X, y)
ridge = Ridge(alpha=1.0).fit(X, y)

print("Linear Regression R²:", lr.score(X, y), "Coefficients:", lr.coef_)
print("Ridge Regression R²:", ridge.score(X, y), "Coefficients:", ridge.coef_)
```

---

### 24. Cross-validation example

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(LinearRegression(), X, y, cv=5, scoring='r2')
print("Cross-validation R² scores:", scores)
print("Mean R²:", scores.mean())
```
"""