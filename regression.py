# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lBfF1ZxlGAlA_wh0oB2FhcqItk5hpTEN

### üîπ **Simple Linear Regression**

 What is Simple Linear Regression?**
It is a statistical method to model the relationship between a dependent variable (Y) and a single independent variable (X) using the equation:
**Y = mX + c**,
where `m` is the slope and `c` is the intercept.

**). What are the key assumptions of Simple Linear Regression?**

1. Linearity between X and Y
2. Homoscedasticity (equal variance of residuals)
3. Independence of observations
4. Normal distribution of residuals

**5. What does the coefficient m represent in the equation Y = mX + c?**
It represents the **slope**, i.e., the change in Y for every one-unit change in X.

**E. What does the intercept c represent in the equation Y = mX + c?**
It is the **value of Y when X = 0**, i.e., the point where the regression line crosses the Y-axis.

**2. How do we calculate the slope m in Simple Linear Regression?**

$$
m = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}
$$

**. What is the purpose of the least squares method in Simple Linear Regression?**
It minimizes the **sum of squared residuals** (differences between observed and predicted Y values) to find the best-fitting line.

**#. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?**
R¬≤ shows the **percentage of variance in Y explained by X**. Ranges from 0 to 1. Higher R¬≤ means better fit.

---

### üîπ **Multiple Linear Regression**

**. What is Multiple Linear Regression?**
It models the relationship between a dependent variable and **two or more independent variables** using the equation:
**Y = b‚ÇÄ + b‚ÇÅX‚ÇÅ + b‚ÇÇX‚ÇÇ + ... + b‚ÇôX‚Çô**

**A. Main difference between Simple and Multiple Linear Regression?**

* Simple: One independent variable
* Multiple: Two or more independent variables

**BD. Key assumptions of Multiple Linear Regression:**

1. Linearity
2. Independence
3. Homoscedasticity
4. No multicollinearity
5. Normal distribution of errors

**BB. What is heteroscedasticity, and how does it affect results?**
It's when residuals have **non-constant variance**. It can lead to unreliable p-values and inefficient estimates.

**B). How can you improve a model with high multicollinearity?**

* Remove highly correlated features
* Use dimensionality reduction (e.g., PCA)
* Apply regularization (Ridge or Lasso)

**B5. Common techniques for transforming categorical variables:**

* **One-Hot Encoding**
* **Label Encoding**
* **Ordinal Encoding**

**BE. Role of interaction terms in Multiple Linear Regression:**
They capture the **combined effect of two or more variables**, which cannot be modeled by individual variables alone.

**B2. How can the interpretation of intercept differ between Simple and Multiple Regression?**
In Simple: Y when X = 0.
In Multiple: Y when all X‚ÇÅ, X‚ÇÇ,...X‚Çô = 0 (may not always be meaningful).

**B. Significance of the slope in regression analysis:**
It shows how much Y changes for a one-unit change in X, holding other variables constant.

**B#. How does the intercept provide context?**
It indicates the baseline value of Y when all independent variables are zero.

**B. Limitations of using R¬≤ as the sole measure:**

* Doesn‚Äôt account for overfitting
* Increases with more predictors even if they add no value
* Doesn‚Äôt measure model accuracy

**BA. How to interpret a large standard error for a regression coefficient?**
It indicates the coefficient estimate is **not reliable**; the variable may not be significant.

**)D. How to identify heteroscedasticity in residual plots?**
If residuals show a pattern (e.g., fan shape), it suggests heteroscedasticity. This can affect model accuracy and inference.

**)B. High R¬≤ but low adjusted R¬≤ means?**
Model may be **overfitting**; some predictors are not adding real value.

**)). Why is scaling important in Multiple Linear Regression?**
To ensure all features contribute equally, especially important for regularization and interpretation.

---

### üîπ **Polynomial Regression**

**)5. What is Polynomial Regression?**
A type of regression that models a **non-linear** relationship between the independent variable and dependent variable using powers of the variable.

**How does it differ from linear regression?**
Linear uses a straight line.
Polynomial allows curves using terms like X¬≤, X¬≥, etc.

**)2. When is Polynomial Regression used?**
When data shows a **non-linear pattern** that linear regression cannot capture.

**). General equation for Polynomial Regression:**

$$
Y = b_0 + b_1X + b_2X^2 + \dots + b_nX^n
$$

**)#. Can polynomial regression be applied to multiple variables?**
Yes, by including **polynomial and interaction terms** for each independent variable.

**). Limitations of Polynomial Regression:**

* Overfitting with high-degree polynomials
* Poor performance outside data range
* Complex interpretation

**)A. Methods to evaluate model fit and select degree:**

* Adjusted R¬≤
* Cross-validation
* AIC/BIC scores
* Visual inspection of fit

**5D. Why is visualization important in Polynomial Regression?**
It helps **understand the curve**, detect **overfitting/underfitting**, and compare model performance visually.

**5B. How is Polynomial Regression implemented in Python?**

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```
"""